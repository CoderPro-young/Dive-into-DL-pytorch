#  梯度消失与梯度爆炸
##  什么是梯度消失与梯度爆炸
梯度消失与梯度爆炸是利用梯度下降法来训练人工神经网络时所出现的一种现象，尤其在神经网络中的隐含层层数非常多时更容易出现这些问题。通常来说，一个神经网络隐含层越多，规模越大，其对于数据集的拟合效果越好，但由于梯度消失与梯度爆炸的存在，会使得靠近输入的隐含层的参数的梯度接近于0（梯度消失的表现）或梯度非常大（梯度爆炸的表现），使得模型非常难以训练，甚至于模型较为复杂的训练效果不及更为简单的模型。
##  梯度消失与梯度爆炸产生的原因
 关于梯度消失与梯度爆炸的原因，我个人认为**神经网络的权重**以及**激活函数**都会对梯度消失或梯度爆炸的产生起到作用,但是网络权重还是作为主要因素。网上有一些回答将梯度消失与梯度爆炸完全归咎于激活函数的选择，个人感觉这还是不准确的，使用Relu函数相比于sigmoid函数或tanh函数确实有利于训练，但是仅仅通过使用Relu函数在训练深层神经网络时还是有可能会遇到梯度消失或梯度爆炸问题，吴恩达在他的课程中解释梯度消失与梯度爆炸问题时甚至是以$g(z)=z$这样的激活函数来举例说明的，下面我们来具体探讨一下这个问题。
###  吴恩达的解释

吴恩达在课程中假设激活函数为$g(z)=z$，我们根据公式$$a^{[l]}=g(W^{[l]}a^{[l-1]})$$可以递推得到$$\hat{y}=\prod_{i=1}^{L}W^{[i]}X$$
这里我们再假设每一层的参数都是相同的，则结果相当于一个矩阵的L次方，这里我们假设每一层的权重为一个标量，则权重只要不为1，经过n次方之后都会趋向于0或者是变得非常大，这就会造成梯度消失或者梯度爆炸问题。
###  笔者的理解


既然是与梯度相关的现象，我们就不妨自己推导一下梯度，看看能得出什么结论。
利用链式法则对第l层的参数求梯度可以得到
$$dw^{[l]}=da^{[L]}\prod_{i=l}^Lg^{'}(z^{[i]})\prod_{i=l+1}^L\frac{\partial{z^{[i]}}}{\partial{a^{i-1}}}\frac{\partial{z^{[l]}}}{\partial{w^{[l]}}}$$
由上面这个式子我们可以直观地看出：激活函数以及每一层的权重均会对梯度造成影响。其中$g^{'}(z^{[i]})$是激活函数的导数值，因此选取Relu函数作为激活函数是有好处的，若每一层输出为正，则这一项为1，不会造成任何影响，而如果选取sigmoid或者tanh则若有一层输出较大时会导致梯度趋向于0，$\frac{\partial{z^{[i]}}}{\partial{a^{[i-1]}}}$的值即为第i层的权重，因此可以看出，仅仅改变激活函数是无法完全克服梯度消失或梯度爆炸的。
以上第二点是笔者个人的理解，公式也是自己推导，如果发现有错误的地方，非常欢迎指正！
